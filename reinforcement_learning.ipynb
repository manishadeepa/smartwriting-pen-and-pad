{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7iAPoQ+1XZlHpf5TqqsPU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manishadeepa/smartwriting-pen-and-pad/blob/main/reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "from IPython.display import display\n",
        "\n",
        "class RLAgent:\n",
        "    def __init__(self, state_space_size, action_space_size, alpha=0.1,\n",
        "                 alpha_min=0.01, alpha_decay=0.995, gamma=0.9, epsilon=0.1,\n",
        "                 epsilon_min=0.01, epsilon_decay=0.995):\n",
        "        self.q_table = np.ones((state_space_size, action_space_size)) * 1.0  # Optimistic initialization\n",
        "        self.alpha = alpha\n",
        "        self.alpha_min = alpha_min\n",
        "        self.alpha_decay = alpha_decay\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.state_space_size = state_space_size\n",
        "        self.action_space_size = action_space_size\n",
        "\n",
        "    def choose_action(self, state_idx):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.randint(0, self.action_space_size - 1)\n",
        "        return np.argmax(self.q_table[state_idx])\n",
        "\n",
        "    def update_q(self, state_idx, action_idx, reward, next_state_idx):\n",
        "        predict = self.q_table[state_idx, action_idx]\n",
        "        target = reward + self.gamma * np.max(self.q_table[next_state_idx])\n",
        "        self.q_table[state_idx, action_idx] += self.alpha * (target - predict)\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def decay_alpha(self):\n",
        "        self.alpha = max(self.alpha_min, self.alpha * self.alpha_decay)\n",
        "\n",
        "    def save(self, path='q_table.pkl'):\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump(self.q_table, f)\n",
        "\n",
        "    def load(self, path='q_table.pkl'):\n",
        "        with open(path, 'rb') as f:\n",
        "            self.q_table = pickle.load(f)\n",
        "\n",
        "    def get_policy(self, state_to_index, actions):\n",
        "        policy = {}\n",
        "        for state, state_idx in state_to_index.items():\n",
        "            best_action_idx = np.argmax(self.q_table[state_idx])\n",
        "            policy[state] = actions[best_action_idx]\n",
        "        return policy\n",
        "\n",
        "# Define states and actions\n",
        "states = [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1),\n",
        " (1, 2), (2, 0), (2, 1), (2, 2)]\n",
        "state_to_index = {s: i for i, s in enumerate(states)}\n",
        "actions = [\"text\", \"visual\", \"audio\", \"simplify\", \"increase\"]\n",
        "\n",
        "def simulate_user_response(state, action):\n",
        "    acc, speed = state\n",
        "    reward = 0\n",
        "\n",
        "    if action == \"simplify\":\n",
        "        if acc == 0:\n",
        "            acc = min(2, acc + 1) if random.random() > 0.3 else acc\n",
        "            reward = 1 if acc > state[0] else -0.5\n",
        "        else:\n",
        "            reward = -0.5\n",
        "    elif action == \"increase\":\n",
        "        if acc == 2:\n",
        "            acc = max(0, acc - 1) if random.random() > 0.5 else acc\n",
        "            reward = -1 if acc < state[0] else 0\n",
        "        else:\n",
        "            acc = min(2, acc + 1) if random.random() > 0.6 else acc\n",
        "            reward = 1 if acc > state[0] else -0.5\n",
        "    elif action == \"visual\":\n",
        "        if speed < 2:\n",
        "            acc = min(2, acc + 1) if random.random() > 0.5 else acc\n",
        "            reward = 1 if acc > state[0] else -0.5\n",
        "        else:\n",
        "            reward = -0.5\n",
        "    elif action == \"audio\":\n",
        "        if speed > 0:\n",
        "            acc = min(2, acc + 1) if random.random() > 0.5 else acc\n",
        "            reward = 1 if acc > state[0] else -0.5\n",
        "        else:\n",
        "            reward = -0.5\n",
        "    else:  # Text\n",
        "        acc = min(2, acc + 1) if random.random() > 0.7 else acc\n",
        "        reward = 0.5 if acc > state[0] else -1\n",
        "\n",
        "    if action in [\"visual\", \"audio\"]:\n",
        "        speed = min(2, max(0, speed + random.choice([-1, 0, 1]) if random.random() > 0.5 else speed))\n",
        "    elif action == \"increase\":\n",
        "        speed = min(2, speed + 1) if random.random() > 0.6 else speed\n",
        "    elif action == \"simplify\":\n",
        "        speed = max(0, speed - 1) if random.random() > 0.6 else speed\n",
        "\n",
        "    return (acc, speed), reward\n",
        "\n",
        "# Training loop\n",
        "agent = RLAgent(state_space_size=len(states), action_space_size=len(actions))\n",
        "eval_rewards = []\n",
        "successes = 0\n",
        "\n",
        "for episode in range(1000):\n",
        "    state = random.choice(states)\n",
        "    state_idx = state_to_index[state]\n",
        "    total_reward = 0\n",
        "    max_steps = 10\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        action_idx = agent.choose_action(state_idx)\n",
        "        action = actions[action_idx]\n",
        "\n",
        "        new_state, reward = simulate_user_response(state, action)\n",
        "        new_state_idx = state_to_index[new_state]\n",
        "\n",
        "        agent.update_q(state_idx, action_idx, reward, new_state_idx)\n",
        "        total_reward += reward\n",
        "\n",
        "        state = new_state\n",
        "        state_idx = new_state_idx\n",
        "\n",
        "        if state[0] == 2:\n",
        "            successes += 1\n",
        "            break\n",
        "\n",
        "    agent.decay_epsilon()\n",
        "    agent.decay_alpha()\n",
        "    eval_rewards.append(total_reward)\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "        avg_reward = np.mean(eval_rewards[-100:]) if eval_rewards else 0\n",
        "        success_rate = successes / (episode + 1)\n",
        "        print(f\"Episode {episode} | Avg Reward: {avg_reward:.2f} | Success Rate: {success_rate:.3f} | Epsilon: {agent.epsilon:.3f} | Alpha: {agent.alpha:.3f}\")\n",
        "\n",
        "agent.save(\"q_table.pkl\")\n",
        "print(\"Q-table saved!\")\n",
        "\n",
        "# Print learned policy\n",
        "policy = agent.get_policy(state_to_index, actions)\n",
        "print(\"\\nLearned Policy:\")\n",
        "for state, action in policy.items():\n",
        "    print(f\"State {state}: {action}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_MQgCvfJpJL",
        "outputId": "13f8224d-ec1d-41de-87d4-a608b695f8f9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0 | Avg Reward: -1.50 | Success Rate: 1.000 | Epsilon: 0.100 | Alpha: 0.100\n",
            "Episode 100 | Avg Reward: 0.17 | Success Rate: 0.990 | Epsilon: 0.060 | Alpha: 0.060\n",
            "Episode 200 | Avg Reward: 0.37 | Success Rate: 0.995 | Epsilon: 0.037 | Alpha: 0.037\n",
            "Episode 300 | Avg Reward: 0.15 | Success Rate: 0.990 | Epsilon: 0.022 | Alpha: 0.022\n",
            "Episode 400 | Avg Reward: 0.38 | Success Rate: 0.990 | Epsilon: 0.013 | Alpha: 0.013\n",
            "Episode 500 | Avg Reward: 0.24 | Success Rate: 0.992 | Epsilon: 0.010 | Alpha: 0.010\n",
            "Episode 600 | Avg Reward: 0.26 | Success Rate: 0.993 | Epsilon: 0.010 | Alpha: 0.010\n",
            "Episode 700 | Avg Reward: 0.35 | Success Rate: 0.994 | Epsilon: 0.010 | Alpha: 0.010\n",
            "Episode 800 | Avg Reward: 0.45 | Success Rate: 0.995 | Epsilon: 0.010 | Alpha: 0.010\n",
            "Episode 900 | Avg Reward: 0.32 | Success Rate: 0.996 | Epsilon: 0.010 | Alpha: 0.010\n",
            "Q-table saved!\n",
            "\n",
            "Learned Policy:\n",
            "State (0, 0): increase\n",
            "State (0, 1): simplify\n",
            "State (0, 2): simplify\n",
            "State (1, 0): visual\n",
            "State (1, 1): audio\n",
            "State (1, 2): audio\n",
            "State (2, 0): visual\n",
            "State (2, 1): text\n",
            "State (2, 2): increase\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for user input\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "class RLAgent:\n",
        "    def __init__(self, state_space_size, action_space_size, q_table_path='q_table.pkl', alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        self.q_table = self._load_q_table(q_table_path)\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.state_space_size = state_space_size\n",
        "        self.action_space_size = action_space_size\n",
        "        self.states = [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
        "        self.state_to_index = {s: i for i, s in enumerate(self.states)}\n",
        "        self.actions = [\"text\", \"visual\", \"audio\", \"simplify\", \"increase\"]\n",
        "\n",
        "    def _load_q_table(self, path):\n",
        "        try:\n",
        "            with open(path, 'rb') as f:\n",
        "                return pickle.load(f)\n",
        "        except FileNotFoundError:\n",
        "            return np.zeros((self.state_space_size, self.action_space_size))\n",
        "\n",
        "    def save_q_table(self, path='q_table.pkl'):\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump(self.q_table, f)\n",
        "\n",
        "    def choose_action(self, state_idx):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.randint(0, self.action_space_size - 1)\n",
        "        return np.argmax(self.q_table[state_idx])\n",
        "\n",
        "    def update_q(self, state_idx, action_idx, reward, next_state_idx):\n",
        "        predict = self.q_table[state_idx, action_idx]\n",
        "        target = reward + self.gamma * np.max(self.q_table[next_state_idx])\n",
        "        self.q_table[state_idx, action_idx] += self.alpha * (target - predict)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if state not in self.state_to_index:\n",
        "            return \"Invalid state\"\n",
        "        state_idx = self.state_to_index[state]\n",
        "        action_idx = self.choose_action(state_idx)\n",
        "        return self.actions[action_idx]\n",
        "\n",
        "    def update_from_feedback(self, state, action, reward, next_state):\n",
        "        state_idx = self.state_to_index[state]\n",
        "        action_idx = self.actions.index(action)\n",
        "        next_state_idx = self.state_to_index[next_state]\n",
        "        self.update_q(state_idx, action_idx, reward, next_state_idx)\n",
        "        self.save_q_table()\n",
        "\n",
        "    def load(self, path='q_table.pkl'):\n",
        "        with open(path, 'rb') as f:\n",
        "            self.q_table = pickle.load(f)"
      ],
      "metadata": {
        "id": "Rc20STMxNd64"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}